{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2fd4e4a-6b58-4b7e-8f4b-814bdaa1d287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/notvisal/Desktop/InternProj/knife_sharpness\n",
      "['.DS_Store', 'Theme2', '.ipynb_checkpoints', 'Better data analysis.ipynb', '.git', 'acceleration_data.csv', 'data.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90cd5c85-b84d-4fb2-9add-8d334d8115fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frame</th>\n",
       "      <th>Label</th>\n",
       "      <th>Pelvis x</th>\n",
       "      <th>Pelvis y</th>\n",
       "      <th>Pelvis z</th>\n",
       "      <th>L5 x</th>\n",
       "      <th>L5 y</th>\n",
       "      <th>L5 z</th>\n",
       "      <th>L3 x</th>\n",
       "      <th>L3 y</th>\n",
       "      <th>...</th>\n",
       "      <th>Left Lower Leg y</th>\n",
       "      <th>Left Lower Leg z</th>\n",
       "      <th>Left Foot x</th>\n",
       "      <th>Left Foot y</th>\n",
       "      <th>Left Foot z</th>\n",
       "      <th>Left Toe x</th>\n",
       "      <th>Left Toe y</th>\n",
       "      <th>Left Toe z</th>\n",
       "      <th>sharpness</th>\n",
       "      <th>sheet_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.640562</td>\n",
       "      <td>-0.345798</td>\n",
       "      <td>-0.666146</td>\n",
       "      <td>-2.369700</td>\n",
       "      <td>-0.499486</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.060481</td>\n",
       "      <td>-1.485525</td>\n",
       "      <td>-2.185359</td>\n",
       "      <td>-1.194785</td>\n",
       "      <td>-1.663235</td>\n",
       "      <td>-2.185359</td>\n",
       "      <td>-1.194785</td>\n",
       "      <td>-1.663235</td>\n",
       "      <td>64</td>\n",
       "      <td>Segment Acceleration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.705326</td>\n",
       "      <td>-0.629461</td>\n",
       "      <td>-0.983502</td>\n",
       "      <td>-1.043948</td>\n",
       "      <td>-0.907626</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.689504</td>\n",
       "      <td>-1.314239</td>\n",
       "      <td>-1.060466</td>\n",
       "      <td>-1.162565</td>\n",
       "      <td>-1.845591</td>\n",
       "      <td>-1.064713</td>\n",
       "      <td>-1.147108</td>\n",
       "      <td>-1.845636</td>\n",
       "      <td>64</td>\n",
       "      <td>Segment Acceleration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.146854</td>\n",
       "      <td>-0.839369</td>\n",
       "      <td>-0.862127</td>\n",
       "      <td>-1.683033</td>\n",
       "      <td>-1.219342</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.178955</td>\n",
       "      <td>-1.321744</td>\n",
       "      <td>-0.220440</td>\n",
       "      <td>-1.530509</td>\n",
       "      <td>-1.658241</td>\n",
       "      <td>-0.227187</td>\n",
       "      <td>-1.514522</td>\n",
       "      <td>-1.657987</td>\n",
       "      <td>64</td>\n",
       "      <td>Segment Acceleration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.065640</td>\n",
       "      <td>-0.431009</td>\n",
       "      <td>-0.488192</td>\n",
       "      <td>-0.426137</td>\n",
       "      <td>-0.574987</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.951495</td>\n",
       "      <td>-1.324380</td>\n",
       "      <td>-0.175790</td>\n",
       "      <td>-1.007643</td>\n",
       "      <td>-1.326566</td>\n",
       "      <td>-0.091278</td>\n",
       "      <td>-0.986575</td>\n",
       "      <td>-1.316862</td>\n",
       "      <td>64</td>\n",
       "      <td>Segment Acceleration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.652927</td>\n",
       "      <td>-0.046127</td>\n",
       "      <td>0.174043</td>\n",
       "      <td>0.920047</td>\n",
       "      <td>-0.080622</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.402592</td>\n",
       "      <td>-0.509365</td>\n",
       "      <td>0.347853</td>\n",
       "      <td>-0.509991</td>\n",
       "      <td>-0.538392</td>\n",
       "      <td>0.335083</td>\n",
       "      <td>-0.493518</td>\n",
       "      <td>-0.537629</td>\n",
       "      <td>64</td>\n",
       "      <td>Segment Acceleration</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 73 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Frame  Label  Pelvis x  Pelvis y  Pelvis z      L5 x      L5 y      L5 z  \\\n",
       "0      0      4       0.0       0.0       0.0 -1.640562 -0.345798 -0.666146   \n",
       "1      1      4       0.0       0.0       0.0 -0.705326 -0.629461 -0.983502   \n",
       "2      2      4       0.0       0.0       0.0 -1.146854 -0.839369 -0.862127   \n",
       "3      3      4       0.0       0.0       0.0 -0.065640 -0.431009 -0.488192   \n",
       "4      4      4       0.0       0.0       0.0  0.652927 -0.046127  0.174043   \n",
       "\n",
       "       L3 x      L3 y  ...  Left Lower Leg y  Left Lower Leg z  Left Foot x  \\\n",
       "0 -2.369700 -0.499486  ...         -2.060481         -1.485525    -2.185359   \n",
       "1 -1.043948 -0.907626  ...         -2.689504         -1.314239    -1.060466   \n",
       "2 -1.683033 -1.219342  ...         -2.178955         -1.321744    -0.220440   \n",
       "3 -0.426137 -0.574987  ...         -0.951495         -1.324380    -0.175790   \n",
       "4  0.920047 -0.080622  ...         -0.402592         -0.509365     0.347853   \n",
       "\n",
       "   Left Foot y  Left Foot z  Left Toe x  Left Toe y  Left Toe z  sharpness  \\\n",
       "0    -1.194785    -1.663235   -2.185359   -1.194785   -1.663235         64   \n",
       "1    -1.162565    -1.845591   -1.064713   -1.147108   -1.845636         64   \n",
       "2    -1.530509    -1.658241   -0.227187   -1.514522   -1.657987         64   \n",
       "3    -1.007643    -1.326566   -0.091278   -0.986575   -1.316862         64   \n",
       "4    -0.509991    -0.538392    0.335083   -0.493518   -0.537629         64   \n",
       "\n",
       "             sheet_type  \n",
       "0  Segment Acceleration  \n",
       "1  Segment Acceleration  \n",
       "2  Segment Acceleration  \n",
       "3  Segment Acceleration  \n",
       "4  Segment Acceleration  \n",
       "\n",
       "[5 rows x 73 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('acceleration_data.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca6ec635-865d-47a5-83bb-8452fe4e827d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151158, 73)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95fbc453-8bbe-49a6-9778-a759f6a0fda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_PATHS = [\n",
    "    'Theme2/P1/Boning/MVN-J-Boning-64-001.xlsx', \n",
    "    'Theme2/P1/Boning/MVN-J-Boning-90-003.xlsx', \n",
    "    'Theme2/P1/Boning/MVN-J-Boning-90-002.xlsx', \n",
    "    'Theme2/P1/Boning/MVN-J-Boning-90-004.xlsx', \n",
    "    'Theme2/P1/Boning/MVN-J-Boning-64-006.xlsx', \n",
    "    'Theme2/P1/Boning/MVN-J-Boning-64-004.xlsx', \n",
    "    'Theme2/P1/Boning/MVN-J-Boning-64-002.xlsx', \n",
    "    'Theme2/P1/Boning/MVN-J-Boning-90-001.xlsx', \n",
    "    'Theme2/P1/Boning/MVN-J-Boning-64-003.xlsx', \n",
    "    'Theme2/P1/Boning/MVN-J-Boning-79-001.xlsx', \n",
    "    'Theme2/P1/Boning/MVN-J-Boning-64-005.xlsx',\n",
    "    'Theme2/P1/Slicing/MVN-J-Slicing-64-001.xlsx', \n",
    "    'Theme2/P1/Slicing/MVN-J-Slicing-87-001.xlsx', \n",
    "    'Theme2/P1/Slicing/MVN-J-Slicing-73-001.xlsx',\n",
    "    'Theme2/P2/Boning/MVN-S-Boning-89-001.xlsx', \n",
    "    'Theme2/P2/Boning/MVN-S-Boning-89-002.xlsx', \n",
    "    'Theme2/P2/Boning/MVN-S-Boning-89-003.xlsx', \n",
    "    'Theme2/P2/Boning/MVN-S-Boning-76-001.xlsx', \n",
    "    'Theme2/P2/Boning/MVN-S-Boning-63-003.xlsx', \n",
    "    'Theme2/P2/Boning/MVN-S-Boning-63-001.xlsx', \n",
    "    'Theme2/P2/Boning/MVN-S-Boning-76-002.xlsx', \n",
    "    'Theme2/P2/Boning/MVN-S-Boning-63-002.xlsx', \n",
    "    'Theme2/P2/Boning/MVN-S-Boning-89-004.xlsx',\n",
    "    'Theme2/P2/Slicing/MVN-S-Slicing-87-001.xlsx', \n",
    "    'Theme2/P2/Slicing/MVN-S-Slicing-73-001.xlsx', \n",
    "    'Theme2/P2/Slicing/MVN-S-Slicing-63-001.xlsx'\n",
    "]\n",
    "\n",
    "ACCELERATION_SHEETS = [\n",
    "    'Segment Acceleration', \n",
    "    'Segment Angular Acceleration'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1774424a-bd80-427f-941e-210945b26fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "\n",
    "# sharpness_levels = [64, 79, 90, 87, 73, 63, 76, 89]\n",
    "\n",
    "# def process_acceleration_data(file_paths, sharpness_levels):\n",
    "#     \"\"\"Process acceleration data from multiple Excel files with different sharpness levels.\"\"\"\n",
    "#     # Dictionary to store processed data by sheet type\n",
    "#     sheet_data = defaultdict(list)\n",
    "    \n",
    "#     # Process all files and extract data by sheet type\n",
    "#     for file_path, sharpness in zip(file_paths, sharpness_levels):\n",
    "#         try:\n",
    "#             xls = pd.ExcelFile(file_path)\n",
    "            \n",
    "#             for sheet_name in xls.sheet_names:\n",
    "#                 if sheet_name not in ACCELERATION_SHEETS:\n",
    "#                     continue\n",
    "                \n",
    "#                 try:\n",
    "#                     # Read the sheet\n",
    "#                     df = pd.read_excel(xls, sheet_name=sheet_name)\n",
    "#                     print(f\"{df.shape}\")\n",
    "                    \n",
    "#                     if df.empty:\n",
    "#                         continue\n",
    "\n",
    "#                     if 'Label' not in df.columns:\n",
    "#                         print(f\"Skipping {sheet_name} since it doesn't have a label column\")\n",
    "#                         continue\n",
    "                    \n",
    "#                     # Add sharpness column\n",
    "#                     df['sharpness'] = sharpness\n",
    "                    \n",
    "#                     # Add sheet name as a column to differentiate data source\n",
    "#                     df['sheet_type'] = sheet_name\n",
    "                    \n",
    "#                     # Add to our collection\n",
    "#                     sheet_data[sheet_name].append(df)\n",
    "                \n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error processing sheet {file_path} {sheet_name}: {e}\")\n",
    "        \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing file {file_path}: {e}\")\n",
    "    \n",
    "#     # Combine data from all files\n",
    "#     combined_dfs = []\n",
    "    \n",
    "#     for sheet_name, dfs in sheet_data.items():\n",
    "#         if dfs:\n",
    "#             # Concatenate all data for this sheet type\n",
    "#             sheet_combined_df = pd.concat(dfs, ignore_index=True)\n",
    "#             combined_dfs.append(sheet_combined_df)\n",
    "    \n",
    "#     # Return combined data frame\n",
    "#     if combined_dfs:\n",
    "#         final_df = pd.concat(combined_dfs, ignore_index=True)\n",
    "#         return final_df\n",
    "    \n",
    "#     return None\n",
    "\n",
    "# def main():\n",
    "#     # Process the data\n",
    "#     merged_df = process_acceleration_data(\n",
    "#         file_paths=ALL_PATHS,\n",
    "#         sharpness_levels=sharpness_levels\n",
    "#     )\n",
    "    \n",
    "#     if merged_df is None or merged_df.empty:\n",
    "#         print(\"No results generated!\")\n",
    "#         return\n",
    "    \n",
    "#     # Save the final merged dataframe\n",
    "#     output_file = \"acceleration_data.csv\"\n",
    "#     merged_df.to_csv(output_file, index=False)\n",
    "#     print(f\"Saved data to {output_file} ({merged_df.shape[0]} rows, {merged_df.shape[1]} columns)\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12cb23ab-2854-4211-b315-9901b7b0022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('acceleration_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41377004-cd94-42d4-8edb-9c4b2b4e401c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151158, 73)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26ef04de-8800-4e12-9023-17d039be69e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_composite_features(df):\n",
    "    \"\"\"\n",
    "    Create composite features from motion capture data - optimized for performance\n",
    "    \"\"\"\n",
    "    # Columns to preserve\n",
    "    preserved_columns = {}\n",
    "    if 'Label' in df.columns:\n",
    "        preserved_columns['Label'] = df['Label']\n",
    "    if 'sharpness' in df.columns:\n",
    "        preserved_columns['sharpness'] = df['sharpness']\n",
    "    \n",
    "    # Get all unique body parts\n",
    "    body_parts = set()\n",
    "    for col in df.columns:\n",
    "        if col.endswith(' x') or col.endswith(' y') or col.endswith(' z'):\n",
    "            body_part = col[:-2]  # Remove the ' x', ' y', or ' z' suffix\n",
    "            body_parts.add(body_part)\n",
    "    \n",
    "    # Dictionary to collect all features\n",
    "    all_features = {}\n",
    "    \n",
    "    # Create aggregated features for each body part\n",
    "    for body_part in body_parts:\n",
    "        # Extract x, y, z components\n",
    "        x_col = f\"{body_part} x\"\n",
    "        y_col = f\"{body_part} y\"\n",
    "        z_col = f\"{body_part} z\"\n",
    "        \n",
    "        # Skip if any component is missing\n",
    "        if not (x_col in df.columns and y_col in df.columns and z_col in df.columns):\n",
    "            continue\n",
    "        \n",
    "        # 1. RMS of x and y (\"mean\")\n",
    "        all_features[f\"{body_part}_mean\"] = np.sqrt((df[x_col]**2 + df[y_col]**2) / 2)\n",
    "        \n",
    "        # 2. RMS of y and z (\"standard deviation\")\n",
    "        all_features[f\"{body_part}_std\"] = np.sqrt((df[y_col]**2 + df[z_col]**2) / 2)\n",
    "        \n",
    "        # 3. RMS of z and x (\"min\")\n",
    "        all_features[f\"{body_part}_min\"] = np.sqrt((df[z_col]**2 + df[x_col]**2) / 2)\n",
    "        \n",
    "        # 4. RMS of x, y, and z (\"max\")\n",
    "        all_features[f\"{body_part}_max\"] = np.sqrt((df[x_col]**2 + df[y_col]**2 + df[z_col]**2) / 3)\n",
    "        \n",
    "        # 5. Roll calculation (\"Area under the curve\")\n",
    "        denominator = np.sqrt(df[x_col]**2 + df[z_col]**2)\n",
    "        denominator = np.where(denominator == 0, 1e-10, denominator)\n",
    "        all_features[f\"{body_part}_AUC\"] = 180 * np.arctan2(df[y_col], denominator) / np.pi\n",
    "        \n",
    "        # 6. Pitch calculation (\"peaks\")\n",
    "        denominator = np.sqrt(df[y_col]**2 + df[z_col]**2)\n",
    "        denominator = np.where(denominator == 0, 1e-10, denominator)\n",
    "        all_features[f\"{body_part}_peaks\"] = 180 * np.arctan2(df[x_col], denominator) / np.pi\n",
    "    \n",
    "    # Combine preserved columns and features\n",
    "    all_features.update(preserved_columns)\n",
    "    \n",
    "    # Create DataFrame in one go\n",
    "    aggregated_features = pd.DataFrame(all_features)\n",
    "    \n",
    "    return aggregated_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "682d081e-aa1a-4ca5-8818-8c3df1805fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_composite_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "039a5511-7591-4998-b920-5f4eddf98ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151158, 140)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8fb61efa-6a01-4001-8e48-024c08948c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training distribution:\n",
      "3_90: 1368\n",
      "4_87: 15523\n",
      "1_87: 2296\n",
      "0_73: 1056\n",
      "2_90: 4862\n",
      "4_90: 9370\n",
      "3_87: 1208\n",
      "2_63: 1173\n",
      "4_76: 3988\n",
      "4_79: 5914\n",
      "0_63: 1592\n",
      "2_89: 1706\n",
      "0_64: 440\n",
      "0_90: 6422\n",
      "3_73: 262\n",
      "2_87: 2436\n",
      "2_64: 1076\n",
      "2_73: 389\n",
      "4_89: 2253\n",
      "2_76: 1144\n",
      "4_63: 1975\n",
      "1_89: 539\n",
      "3_63: 320\n",
      "5_79: 190\n",
      "0_87: 548\n",
      "4_73: 1933\n",
      "4_64: 1476\n",
      "5_63: 520\n",
      "0_89: 1646\n",
      "5_87: 946\n",
      "2_79: 769\n",
      "1_90: 2162\n",
      "3_64: 318\n",
      "0_76: 874\n",
      "5_90: 1301\n",
      "3_76: 317\n",
      "1_76: 384\n",
      "1_63: 99\n",
      "0_79: 804\n",
      "1_79: 708\n",
      "5_76: 691\n",
      "3_89: 430\n",
      "3_79: 443\n",
      "5_73: 308\n",
      "5_89: 179\n",
      "1_73: 290\n",
      "\n",
      "Sampling strategy dictionary:\n",
      "3_90: 11642\n",
      "1_87: 11642\n",
      "0_73: 11642\n",
      "2_90: 11642\n",
      "4_90: 11642\n",
      "3_87: 11642\n",
      "2_63: 11642\n",
      "4_76: 11642\n",
      "4_79: 11642\n",
      "0_63: 11642\n",
      "2_89: 11642\n",
      "0_64: 11642\n",
      "0_90: 11642\n",
      "3_73: 11642\n",
      "2_87: 11642\n",
      "2_64: 11642\n",
      "2_73: 11642\n",
      "4_89: 11642\n",
      "2_76: 11642\n",
      "4_63: 11642\n",
      "1_89: 11642\n",
      "3_63: 11642\n",
      "5_79: 11642\n",
      "0_87: 11642\n",
      "4_73: 11642\n",
      "4_64: 11642\n",
      "5_63: 11642\n",
      "0_89: 11642\n",
      "5_87: 11642\n",
      "2_79: 11642\n",
      "1_90: 11642\n",
      "3_64: 11642\n",
      "0_76: 11642\n",
      "5_90: 11642\n",
      "3_76: 11642\n",
      "1_76: 11642\n",
      "1_63: 11642\n",
      "0_79: 11642\n",
      "1_79: 11642\n",
      "5_76: 11642\n",
      "3_89: 11642\n",
      "3_79: 11642\n",
      "5_73: 11642\n",
      "5_89: 11642\n",
      "1_73: 11642\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 119\u001b[39m\n\u001b[32m    112\u001b[39m smote_enn = SMOTEENN(\n\u001b[32m    113\u001b[39m     smote=smote_nc,\n\u001b[32m    114\u001b[39m     random_state=\u001b[32m42\u001b[39m,\n\u001b[32m    115\u001b[39m     sampling_strategy=sampling_strategy_dict\n\u001b[32m    116\u001b[39m )\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# Apply SMOTEENN on the training set only.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m X_resampled, y_resampled = \u001b[43msmote_enn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_features_for_sampling\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcombined_target\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# Extract the activity label from the combined target.\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# (Assuming the format is \"activity_sharpness\", and activity is the first part.)\u001b[39;00m\n\u001b[32m    126\u001b[39m y_resampled_activity = np.array([\u001b[38;5;28mint\u001b[39m(label.split(\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m)[\u001b[32m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m y_resampled])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/myenv/lib/python3.12/site-packages/imblearn/base.py:202\u001b[39m, in \u001b[36mBaseSampler.fit_resample\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_resample\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, **params):\n\u001b[32m    182\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[32m    183\u001b[39m \n\u001b[32m    184\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    200\u001b[39m \u001b[33;03m        The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/myenv/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/myenv/lib/python3.12/site-packages/imblearn/base.py:105\u001b[39m, in \u001b[36mSamplerMixin.fit_resample\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m     99\u001b[39m X, y, binarize_y = \u001b[38;5;28mself\u001b[39m._check_X_y(X, y)\n\u001b[32m    101\u001b[39m \u001b[38;5;28mself\u001b[39m.sampling_strategy_ = check_sampling_strategy(\n\u001b[32m    102\u001b[39m     \u001b[38;5;28mself\u001b[39m.sampling_strategy, y, \u001b[38;5;28mself\u001b[39m._sampling_type\n\u001b[32m    103\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m y_ = (\n\u001b[32m    108\u001b[39m     label_binarize(output[\u001b[32m1\u001b[39m], classes=np.unique(y)) \u001b[38;5;28;01mif\u001b[39;00m binarize_y \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[32m1\u001b[39m]\n\u001b[32m    109\u001b[39m )\n\u001b[32m    111\u001b[39m X_, y_ = arrays_transformer.transform(output[\u001b[32m0\u001b[39m], y_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/myenv/lib/python3.12/site-packages/imblearn/combine/_smote_enn.py:160\u001b[39m, in \u001b[36mSMOTEENN._fit_resample\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28mself\u001b[39m.sampling_strategy_ = \u001b[38;5;28mself\u001b[39m.sampling_strategy\n\u001b[32m    159\u001b[39m X_res, y_res = \u001b[38;5;28mself\u001b[39m.smote_.fit_resample(X, y)\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menn_\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_res\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_res\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/myenv/lib/python3.12/site-packages/imblearn/base.py:202\u001b[39m, in \u001b[36mBaseSampler.fit_resample\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_resample\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, **params):\n\u001b[32m    182\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[32m    183\u001b[39m \n\u001b[32m    184\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    200\u001b[39m \u001b[33;03m        The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/myenv/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/myenv/lib/python3.12/site-packages/imblearn/base.py:105\u001b[39m, in \u001b[36mSamplerMixin.fit_resample\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m     99\u001b[39m X, y, binarize_y = \u001b[38;5;28mself\u001b[39m._check_X_y(X, y)\n\u001b[32m    101\u001b[39m \u001b[38;5;28mself\u001b[39m.sampling_strategy_ = check_sampling_strategy(\n\u001b[32m    102\u001b[39m     \u001b[38;5;28mself\u001b[39m.sampling_strategy, y, \u001b[38;5;28mself\u001b[39m._sampling_type\n\u001b[32m    103\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m y_ = (\n\u001b[32m    108\u001b[39m     label_binarize(output[\u001b[32m1\u001b[39m], classes=np.unique(y)) \u001b[38;5;28;01mif\u001b[39;00m binarize_y \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[32m1\u001b[39m]\n\u001b[32m    109\u001b[39m )\n\u001b[32m    111\u001b[39m X_, y_ = arrays_transformer.transform(output[\u001b[32m0\u001b[39m], y_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/myenv/lib/python3.12/site-packages/imblearn/under_sampling/_prototype_selection/_edited_nearest_neighbours.py:168\u001b[39m, in \u001b[36mEditedNearestNeighbours._fit_resample\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    166\u001b[39m X_class = _safe_indexing(X, target_class_indices)\n\u001b[32m    167\u001b[39m y_class = _safe_indexing(y, target_class_indices)\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m nnhood_idx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnn_\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[:, \u001b[32m1\u001b[39m:]\n\u001b[32m    169\u001b[39m nnhood_label = y[nnhood_idx]\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.kind_sel == \u001b[33m\"\u001b[39m\u001b[33mmode\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/myenv/lib/python3.12/site-packages/sklearn/neighbors/_base.py:869\u001b[39m, in \u001b[36mKNeighborsMixin.kneighbors\u001b[39m\u001b[34m(self, X, n_neighbors, return_distance)\u001b[39m\n\u001b[32m    862\u001b[39m use_pairwise_distances_reductions = (\n\u001b[32m    863\u001b[39m     \u001b[38;5;28mself\u001b[39m._fit_method == \u001b[33m\"\u001b[39m\u001b[33mbrute\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    864\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m ArgKmin.is_usable_for(\n\u001b[32m    865\u001b[39m         X \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit_X, \u001b[38;5;28mself\u001b[39m._fit_X, \u001b[38;5;28mself\u001b[39m.effective_metric_\n\u001b[32m    866\u001b[39m     )\n\u001b[32m    867\u001b[39m )\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_pairwise_distances_reductions:\n\u001b[32m--> \u001b[39m\u001b[32m869\u001b[39m     results = \u001b[43mArgKmin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meffective_metric_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meffective_metric_params_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m    880\u001b[39m     \u001b[38;5;28mself\u001b[39m._fit_method == \u001b[33m\"\u001b[39m\u001b[33mbrute\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.metric == \u001b[33m\"\u001b[39m\u001b[33mprecomputed\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m issparse(X)\n\u001b[32m    881\u001b[39m ):\n\u001b[32m    882\u001b[39m     results = _kneighbors_from_graph(\n\u001b[32m    883\u001b[39m         X, n_neighbors=n_neighbors, return_distance=return_distance\n\u001b[32m    884\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/myenv/lib/python3.12/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py:281\u001b[39m, in \u001b[36mArgKmin.compute\u001b[39m\u001b[34m(cls, X, Y, k, metric, chunk_size, metric_kwargs, strategy, return_distance)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Compute the argkmin reduction.\u001b[39;00m\n\u001b[32m    201\u001b[39m \n\u001b[32m    202\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    278\u001b[39m \u001b[33;03mreturns.\u001b[39;00m\n\u001b[32m    279\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m X.dtype == Y.dtype == np.float64:\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mArgKmin64\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[43m=\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m X.dtype == Y.dtype == np.float32:\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ArgKmin32.compute(\n\u001b[32m    294\u001b[39m         X=X,\n\u001b[32m    295\u001b[39m         Y=Y,\n\u001b[32m   (...)\u001b[39m\u001b[32m    301\u001b[39m         return_distance=return_distance,\n\u001b[32m    302\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32msklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx:59\u001b[39m, in \u001b[36msklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.compute\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/myenv/lib/python3.12/site-packages/threadpoolctl.py:592\u001b[39m, in \u001b[36m_ThreadpoolLimiter.__exit__\u001b[39m\u001b[34m(self, type, value, traceback)\u001b[39m\n\u001b[32m    589\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    590\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m592\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mtype\u001b[39m, value, traceback):\n\u001b[32m    593\u001b[39m     \u001b[38;5;28mself\u001b[39m.restore_original_limits()\n\u001b[32m    595\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    596\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrap\u001b[39m(\u001b[38;5;28mcls\u001b[39m, controller, *, limits=\u001b[38;5;28;01mNone\u001b[39;00m, user_api=\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Conv1D, MaxPool1D, GlobalAveragePooling1D, BatchNormalization, Dense, Activation\n",
    "\n",
    "# Assume df is your original dataframe with shape (150000, 140)\n",
    "# and that it contains the columns 'Label' (activity), 'sharpness', and other numeric features.\n",
    "# The combined target (for oversampling) is built as: activity_sharpness.\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Split Original Data into Train+Val and Test Sets\n",
    "# ------------------------------\n",
    "train_val_dataset = df.sample(frac=0.7, random_state=42)\n",
    "test_dataset = df.drop(train_val_dataset.index)\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Split Train+Val into Training and Validation Sets\n",
    "# ------------------------------\n",
    "# Stratify by 'Label' to preserve class distribution.\n",
    "train_dataset, val_dataset = train_test_split(\n",
    "    train_val_dataset, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=train_val_dataset['Label']\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# (A) Prepare RAW Training Data (No Oversampling)\n",
    "# ===============================\n",
    "# Create raw training features and labels.\n",
    "raw_train_features = train_dataset.copy()  # copy all columns\n",
    "raw_train_labels = raw_train_features.pop('Label')  # remove 'Label' column\n",
    "\n",
    "# List of numeric columns (all columns of interest, including 'sharpness' etc.)\n",
    "numeric_cols = df.select_dtypes(include=['int', 'int64', 'float', 'float64']).columns.tolist()\n",
    "\n",
    "# Ensure our raw training features include the columns we need.\n",
    "# (They should, since train_dataset was a subset of df.)\n",
    "\n",
    "# Define the numeric columns to be scaled.\n",
    "# (For scaling, exclude columns that you don't want to scale; here we exclude 'Label' and 'sharpness' if needed.)\n",
    "numerical_cols_for_scaling = [col for col in numeric_cols if col not in ['Label', 'sharpness']]\n",
    "\n",
    "# Initialize and fit the scaler on raw training data.\n",
    "scaler = RobustScaler()\n",
    "raw_train_features[numerical_cols_for_scaling] = scaler.fit_transform(raw_train_features[numerical_cols_for_scaling])\n",
    "\n",
    "# (Optionally, reshape raw training data if you plan to use it with a model that expects a 3D tensor.\n",
    "# For example, if each sample should be interpreted as a sequence of length = number of columns.)\n",
    "num_time_steps = raw_train_features.shape[1]  # this is typically 140 (or 139 if one column was Label)\n",
    "X_raw_train = raw_train_features.values.reshape(-1, num_time_steps, 1)\n",
    "y_raw_train = tf.keras.utils.to_categorical(raw_train_labels.values, num_classes=46)\n",
    "\n",
    "# ===============================\n",
    "# (B) Prepare Oversampled Training Data Using SMOTENC + SMOTEENN\n",
    "# ===============================\n",
    "\n",
    "# Prepare training features for oversampling from the original train_dataset.\n",
    "# (We work on a separate copy so that our raw training data remains unchanged.)\n",
    "train_features = train_dataset[numeric_cols].copy()\n",
    "train_labels = train_features.pop('Label')\n",
    "\n",
    "# Create a combined target (e.g., \"activity_sharpness\") for oversampling.\n",
    "train_features['combined_target'] = train_labels.astype(str) + '_' + train_features['sharpness'].astype(str)\n",
    "\n",
    "# For oversampling, drop the combined target from the features.\n",
    "train_features_for_sampling = train_features.drop(['combined_target'], axis=1)\n",
    "\n",
    "# Scale the training features for oversampling using the same columns.\n",
    "# (We use the same numerical_cols_for_scaling as before.)\n",
    "train_features_for_sampling[numerical_cols_for_scaling] = scaler.transform(train_features_for_sampling[numerical_cols_for_scaling])\n",
    "\n",
    "# Get the combined target for oversampling.\n",
    "combined_target = train_features['combined_target'].copy()\n",
    "\n",
    "# Identify categorical feature indices for SMOTENC (e.g., for 'Label' and 'sharpness').\n",
    "categorical_features = [\n",
    "    train_features_for_sampling.columns.get_loc(col)\n",
    "    for col in train_features_for_sampling.columns if col in ['Label', 'sharpness']\n",
    "]\n",
    "\n",
    "# Display original distribution for training (before oversampling).\n",
    "class_counts = Counter(combined_target)\n",
    "print(\"Original training distribution:\")\n",
    "for key, count in class_counts.items():\n",
    "    print(f\"{key}: {count}\")\n",
    "\n",
    "# Compute target count as 75% of the majority class count.\n",
    "majority_class_count = max(class_counts.values())\n",
    "target_count = int(0.75 * majority_class_count)\n",
    "\n",
    "# Build a sampling strategy dictionary for minority classes only.\n",
    "sampling_strategy_dict = {label: target_count for label, count in class_counts.items() if count < target_count}\n",
    "print(\"\\nSampling strategy dictionary:\")\n",
    "for key, val in sampling_strategy_dict.items():\n",
    "    print(f\"{key}: {val}\")\n",
    "\n",
    "# Initialize SMOTENC.\n",
    "smote_nc = SMOTENC(\n",
    "    categorical_features=categorical_features,\n",
    "    random_state=42,\n",
    "    k_neighbors=5,\n",
    ")\n",
    "\n",
    "# Initialize SMOTEENN using the SMOTENC oversampler and our custom sampling strategy.\n",
    "smote_enn = SMOTEENN(\n",
    "    smote=smote_nc,\n",
    "    random_state=42,\n",
    "    sampling_strategy=sampling_strategy_dict\n",
    ")\n",
    "\n",
    "# Apply SMOTEENN on the training set only.\n",
    "X_resampled, y_resampled = smote_enn.fit_resample(\n",
    "    train_features_for_sampling.values, \n",
    "    combined_target.values\n",
    ")\n",
    "\n",
    "# Extract the activity label from the combined target.\n",
    "# (Assuming the format is \"activity_sharpness\", and activity is the first part.)\n",
    "y_resampled_activity = np.array([int(label.split('_')[0]) for label in y_resampled])\n",
    "\n",
    "# Convert the oversampled features back into a DataFrame.\n",
    "resampled_train_df = pd.DataFrame(X_resampled, columns=train_features_for_sampling.columns)\n",
    "resampled_train_df['Label'] = y_resampled_activity\n",
    "\n",
    "print(\"\\nOversampled training Label distribution:\")\n",
    "print(Counter(resampled_train_df['Label']))\n",
    "\n",
    "# If needed, reshape oversampled training data for your model.\n",
    "num_time_steps_resampled = resampled_train_df.shape[1]  # should be same as num_time_steps\n",
    "X_oversampled_train = resampled_train_df[numeric_cols].values.reshape(-1, num_time_steps_resampled, 1)\n",
    "y_oversampled_train = tf.keras.utils.to_categorical(resampled_train_df['Label'].values, num_classes=46)\n",
    "\n",
    "# =====================================================\n",
    "# Summary of available training data variables:\n",
    "#   - Raw training data:\n",
    "#         X_raw_train, y_raw_train\n",
    "#   - Oversampled training data:\n",
    "#         X_oversampled_train, y_oversampled_train\n",
    "# =====================================================\n",
    "\n",
    "# You can now choose which one to use for training.\n",
    "# For example, to train on raw data:\n",
    "# model.fit(X_raw_train, y_raw_train, ...)\n",
    "# Or, to train on oversampled data:\n",
    "# model.fit(X_oversampled_train, y_oversampled_train, ...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78352933-f790-49b7-b3f9-14688df04b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the class distribution after resampling\n",
    "unique_values, counts = np.unique(y_resampled, return_counts=True)\n",
    "class_distribution = dict(zip(unique_values, counts))\n",
    "print(\"\\nClass distribution after SMOTE:\")\n",
    "for class_value, count in sorted(class_distribution.items()):\n",
    "    print(f\"activity label {class_value}: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dfdb85-7932-4a29-95d5-3feb47dc1884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def correlation_based_selection(df, threshold=0.8):\n",
    "#     \"\"\"\n",
    "#     Remove highly correlated features using a correlation threshold.\n",
    "    \n",
    "#     Args:\n",
    "#         df: DataFrame containing features\n",
    "#         threshold: Correlation threshold above which to remove features\n",
    "        \n",
    "#     Returns:\n",
    "#         List of features to keep\n",
    "#     \"\"\"\n",
    "#     corr_matrix = df.corr().abs()\n",
    "#     upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    \n",
    "#     # Identify features to remove\n",
    "#     to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    \n",
    "#     # Features to keep\n",
    "#     to_keep = [column for column in df.columns if column not in to_drop]\n",
    "    \n",
    "#     return to_keep\n",
    "\n",
    "# def hybrid_dimensionality_reduction(df, target='Label', n_features=30):\n",
    "#     \"\"\"\n",
    "#     Hybrid approach to feature selection.\n",
    "    \n",
    "#     Args:\n",
    "#         df: DataFrame with features\n",
    "#         target: Target column name\n",
    "#         n_features: Target number of features\n",
    "        \n",
    "#     Returns:\n",
    "#         List of selected features\n",
    "#     \"\"\"\n",
    "#     low_corr_features = correlation_based_selection(df.drop(columns=[target]), threshold=0.99)\n",
    "    \n",
    "#     X = df[low_corr_features]\n",
    "#     y = df[target]\n",
    "    \n",
    "#     # Calculate mutual information scores\n",
    "#     mi_scores = mutual_info_classif(X, y)\n",
    "#     mi_features = pd.DataFrame({'feature': low_corr_features, 'importance': mi_scores})\n",
    "#     mi_features = mi_features.sort_values('importance', ascending=False)\n",
    "    \n",
    "#     # Define anatomical groups based on your feature names\n",
    "#     anatomical_groups = {\n",
    "#         'spine': [col for col in low_corr_features if any(x in col for x in ['Pelvis', 'L5', 'L3', 'T12', 'T8', 'Neck'])],\n",
    "#         'head': [col for col in low_corr_features if 'Head' in col],\n",
    "#         'right_arm': [col for col in low_corr_features if any(x in col for x in ['Right Hand', 'Right Forearm', 'Right Upper Arm', 'Right Shoulder'])],\n",
    "#         'left_arm': [col for col in low_corr_features if any(x in col for x in ['Left Hand', 'Left Forearm', 'Left Upper Arm', 'Left Shoulder'])],\n",
    "#         'right_leg': [col for col in low_corr_features if any(x in col for x in ['Right Upper Leg', 'Right Lower Leg', 'Right Foot', 'Right Toe'])],\n",
    "#         'left_leg': [col for col in low_corr_features if any(x in col for x in ['Left Upper Leg', 'Left Lower Leg', 'Left Foot', 'Left Toe'])]\n",
    "#     }\n",
    "    \n",
    "#     # Ensure at least one feature from each group\n",
    "#     must_include = []\n",
    "#     for group, features in anatomical_groups.items():\n",
    "#         if features:\n",
    "#             # Get feature with highest MI score from this group\n",
    "#             group_features = mi_features[mi_features['feature'].isin(features)]\n",
    "#             if not group_features.empty:\n",
    "#                 top_feature = group_features.iloc[0]['feature']\n",
    "#                 must_include.append(top_feature)\n",
    "    \n",
    "#     # Add remaining top features by MI score until we reach n_features\n",
    "#     top_by_mi = mi_features[~mi_features['feature'].isin(must_include)]['feature'].tolist()\n",
    "#     selected = must_include + top_by_mi[:n_features - len(must_include)]\n",
    "    \n",
    "#     return selected[:n_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad3bdaf-f422-4c40-b86f-abfe22b7daea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sharpness_col = df.pop('sharpness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2e7e3e-3539-4d60-9a6c-d5eb0926bd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop('sharpness', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b89b23e-a342-4b95-9168-2c9d8e174958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# feature_counts = [30, 50, 80, 90, 100, 120, 130, 140]\n",
    "# reduced_dfs = []\n",
    "# for n in feature_counts:\n",
    "#     selected_features = hybrid_dimensionality_reduction(df, target='Label', n_features=n)\n",
    "#     reduced_df = pd.DataFrame(df[selected_features + ['Label']])\n",
    "#     reduced_dfs.append(reduced_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dd916b-2c8e-4f65-ace6-423c2fdeca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import train_test_split, cross_val_score\n",
    "# from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Store results for comparison\n",
    "# results = {\n",
    "#     'feature_count': [],\n",
    "#     'accuracy': [],\n",
    "#     'f1_macro': [],\n",
    "#     'model': [],\n",
    "#     'confusion_matrix': []\n",
    "# }\n",
    "\n",
    "# # Iterate through each reduced DataFrame\n",
    "# for i, n_features in enumerate(feature_counts):\n",
    "#     print(f\"\\n{'='*50}\")\n",
    "#     print(f\"Evaluating model with {n_features} features {reduced_dfs[i].shape}\")\n",
    "#     print(f\"{'='*50}\")\n",
    "    \n",
    "#     # Get the current reduced DataFrame\n",
    "#     df = reduced_dfs[i]\n",
    "    \n",
    "#     # Split features and target\n",
    "#     X = df.drop('Label', axis=1)\n",
    "#     y = df['Label']\n",
    "    \n",
    "#     # Split into train and test sets (70/30 split)\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(\n",
    "#         X, y, test_size=0.3, random_state=42, stratify=y\n",
    "#     )\n",
    "    \n",
    "#     # Normalize features using StandardScaler\n",
    "#     scaler = StandardScaler()\n",
    "#     X_train_scaled = scaler.fit_transform(X_train)\n",
    "#     X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "#     # Train a Random Forest model\n",
    "#     rf = RandomForestClassifier(\n",
    "#         n_estimators=50, \n",
    "#         max_depth=None,\n",
    "#         min_samples_split=2,\n",
    "#         min_samples_leaf=1,\n",
    "#         random_state=42,\n",
    "#         n_jobs=-1  # Use all CPU cores\n",
    "#     )\n",
    "    \n",
    "#     # Fit the model\n",
    "#     rf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "#     # Make predictions\n",
    "#     y_pred = rf.predict(X_test_scaled)\n",
    "    \n",
    "#     # Calculate performance metrics\n",
    "#     accuracy = accuracy_score(y_test, y_pred)\n",
    "#     f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "#     # Also perform 5-fold cross-validation\n",
    "#     cv_scores = cross_val_score(rf, X, y, cv=5, scoring='accuracy')\n",
    "    \n",
    "#     # Save results\n",
    "#     results['feature_count'].append(n_features)\n",
    "#     results['accuracy'].append(accuracy)\n",
    "#     results['f1_macro'].append(f1)\n",
    "#     results['model'].append(rf)\n",
    "    \n",
    "#     # Generate confusion matrix (raw counts)\n",
    "#     cm = confusion_matrix(y_test, y_pred)\n",
    "#     results['confusion_matrix'].append(cm)\n",
    "    \n",
    "#     # Display results\n",
    "#     print(f\"Model with {n_features} features:\")\n",
    "#     print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "#     print(f\"F1 Score (Macro): {f1:.4f}\")\n",
    "#     print(f\"Cross-validation Accuracy: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}\")\n",
    "    \n",
    "#     # Display feature importances\n",
    "#     feature_importance = pd.DataFrame({\n",
    "#         'Feature': X.columns,\n",
    "#         'Importance': rf.feature_importances_\n",
    "#     }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "#     print(\"\\nTop 10 important features:\")\n",
    "#     print(feature_importance.head(10))\n",
    "    \n",
    "#     # Plot confusion matrix with raw counts\n",
    "#     plt.figure(figsize=(10, 8))\n",
    "#     sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "#                 xticklabels=sorted(y.unique()),\n",
    "#                 yticklabels=sorted(y.unique()))\n",
    "#     plt.xlabel('Predicted Label')\n",
    "#     plt.ylabel('True Label')\n",
    "#     plt.title(f'Confusion Matrix - {n_features} Features')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83693b0d-8b91-4c54-911d-94224dafb789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot accuracy comparison across different feature counts\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(results['feature_count'], results['accuracy'], 'o-', label='Accuracy')\n",
    "# plt.plot(results['feature_count'], results['f1_macro'], 's-', label='F1 Score (Macro)')\n",
    "# plt.xlabel('Number of Features')\n",
    "# plt.ylabel('Score')\n",
    "# plt.title('Model Performance vs. Number of Features')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# # Identify the best performing model\n",
    "# best_idx = np.argmax(results['accuracy'])\n",
    "# best_feature_count = results['feature_count'][best_idx]\n",
    "# best_accuracy = results['accuracy'][best_idx]\n",
    "# best_f1 = results['f1_macro'][best_idx]\n",
    "\n",
    "# print(f\"\\nBest model has {best_feature_count} features\")\n",
    "# print(f\"Best accuracy: {best_accuracy:.4f}\")\n",
    "# print(f\"Best F1 score: {best_f1:.4f}\")\n",
    "\n",
    "# # Feature importance analysis of the best model\n",
    "# best_model = results['model'][best_idx]\n",
    "# best_features = reduced_dfs[best_idx].drop('Label', axis=1).columns\n",
    "# best_importances = best_model.feature_importances_\n",
    "\n",
    "# # Plot feature importances for the best model\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# feature_importance = pd.DataFrame({\n",
    "#     'Feature': best_features,\n",
    "#     'Importance': best_importances\n",
    "# }).sort_values('Importance', ascending=False)\n",
    "\n",
    "# sns.barplot(x='Importance', y='Feature', data=feature_importance.head(20))\n",
    "# plt.title(f'Top 20 Feature Importances for Best Model ({best_feature_count} features)')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad103629-6a8b-41ae-8f20-269e7331ce1c",
   "metadata": {},
   "source": [
    "# **Hybrid Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0353ec8d-541c-49eb-a005-811d11c07925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, LSTM, Conv1D, MaxPool1D, GlobalAveragePooling1D, BatchNormalization, Dense, Activation, Reshape\n",
    "# For Validation Set:\n",
    "val_features = val_dataset[numeric_cols].copy()\n",
    "val_labels = val_features.pop('Label')\n",
    "# Apply the same scaling to numerical columns in the validation set\n",
    "val_features[numerical_cols] = scaler.transform(val_features[numerical_cols])\n",
    "# Use the actual number of columns from the DataFrame for reshaping\n",
    "num_time_steps_val = val_features.shape[1]\n",
    "X_val_model = val_features.values.reshape(-1, num_time_steps_val, 1)\n",
    "y_val_model = tf.keras.utils.to_categorical(val_labels.values, num_classes=46)\n",
    "\n",
    "# For Test Set:\n",
    "test_features = test_dataset[numeric_cols].copy()\n",
    "test_labels = test_features.pop('Label')\n",
    "test_features[numerical_cols] = scaler.transform(test_features[numerical_cols])\n",
    "num_time_steps_test = test_features.shape[1]\n",
    "X_test_model = test_features.values.reshape(-1, num_time_steps_test, 1)\n",
    "y_test_model = tf.keras.utils.to_categorical(test_labels.values, num_classes=46)\n",
    "\n",
    "# For Training Set (after oversampling):\n",
    "# Note: Here, numeric_cols is used to reconstruct the training features.\n",
    "X_train_model = resampled_train_df[numeric_cols].values.reshape(-1, resampled_train_df[numeric_cols].shape[1], 1)\n",
    "y_train_model = tf.keras.utils.to_categorical(resampled_train_df['Label'].values, num_classes=46)\n",
    "\n",
    "# Prepare RAW Training Data from train_dataset\n",
    "raw_train_features = train_dataset[numeric_cols].copy()\n",
    "raw_train_labels = raw_train_features.pop('Label')\n",
    "\n",
    "# Apply scaling to the raw training features using the same scaler\n",
    "raw_train_features[numeric_cols] = scaler.transform(raw_train_features[numeric_cols])\n",
    "\n",
    "# Determine the actual number of features (time steps) and reshape accordingly\n",
    "num_time_steps_raw = raw_train_features.shape[1]\n",
    "X_raw_train = raw_train_features.values.reshape(-1, num_time_steps_raw, 1)\n",
    "y_raw_train = tf.keras.utils.to_categorical(raw_train_labels.values, num_classes=46)\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Create the EarlyStopping callback: monitor validation loss, with a patience of 3 epochs.\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# ------------------------------\n",
    "# Build the Activity Recognition Model\n",
    "# ------------------------------\n",
    "time_steps = 140       # Sequence length (from your 140 features)\n",
    "num_features = 1       # 1 feature per time step\n",
    "num_classes = 46       # 46 combined classes (activity classes)\n",
    "\n",
    "model = Sequential()\n",
    "# First LSTM with dropout after it\n",
    "model.add(LSTM(32, return_sequences=True, input_shape=(time_steps, num_features), activation='relu'))\n",
    "model.add(Dropout(0.2))  # 20% dropout\n",
    "# Second LSTM with dropout after it\n",
    "model.add(LSTM(32, return_sequences=True, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "# Convolutional block\n",
    "model.add(Conv1D(filters=64, kernel_size=2, activation='relu', strides=2))\n",
    "model.add(MaxPool1D(pool_size=4, padding='same'))\n",
    "model.add(Conv1D(filters=192, kernel_size=2, activation='relu', strides=1))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "# Optionally, add dropout here as well if needed\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization(epsilon=1e-06))\n",
    "# Final classification layer\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# ------------------------------\n",
    "# Compile and Train the Model with a Validation Set and EarlyStopping\n",
    "# ------------------------------\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    X_raw_train, y_raw_train,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    verbose=1,\n",
    "    validation_data=(X_val_model, y_val_model),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# Evaluate on the Test Set\n",
    "# ------------------------------\n",
    "test_loss, test_accuracy = model.evaluate(X_test_model, y_test_model, verbose=0)\n",
    "print(\"Test Accuracy: {:.2f}%\".format(test_accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aa236b-0900-4cd8-8c67-adaa7138f745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Predict class probabilities on the test set\n",
    "y_pred_probs = model.predict(X_test_model)\n",
    "# Convert probabilities to class predictions\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "# Convert one-hot encoded true labels to class indices\n",
    "y_true = np.argmax(y_test_model, axis=1)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Display the confusion matrix using scikit-learn's built-in display\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(ax=ax, cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94762f07-cb71-419a-b9c1-89ea5bef533d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06818121-bc1a-4317-a38c-c83f0bbbd918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
